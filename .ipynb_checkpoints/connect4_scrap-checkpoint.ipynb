{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, BatchNormalization, Activation, add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class AlphaZeroConfig:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Self-Play\n",
    "        self.self_play_games = 25\n",
    "\n",
    "        self.num_sampling_moves = 8\n",
    "        self.num_simulations = 25\n",
    "\n",
    "        # Root prior exploration noise.\n",
    "        self.root_dirichlet_alpha = 1\n",
    "        self.root_exploration_fraction = 0.25\n",
    "\n",
    "        # UCB formula\n",
    "        self.pb_c_base = 19652\n",
    "        self.pb_c_init = 1.25\n",
    "\n",
    "        # Training\n",
    "        self.batches_per_iter = 5\n",
    "        self.epochs_per_batch = 5\n",
    "        self.learning_rate = 1e-3\n",
    "        self.window_size = 500\n",
    "        self.batch_size = 4096\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, config=None):\n",
    "        self.batch_size = 4096\n",
    "        self.buffer = deque(maxlen=500)\n",
    "        if config is not None:\n",
    "            self.batch_size = config.batch_size\n",
    "            self.buffer = deque(maxlen=config.window_size)\n",
    "\n",
    "    def save_game(self, game):\n",
    "        self.buffer.append(game)\n",
    "\n",
    "    def sample_batch(self):\n",
    "        move_sum = float(sum(len(g.history) for g in self.buffer))\n",
    "\n",
    "        games = np.random.choice(self.buffer, size=self.batch_size,\n",
    "                                 p=[len(g.history) / move_sum for g in self.buffer])\n",
    "        game_pos = [(g, np.random.randint(len(g.history))) for g in games]\n",
    "        # batch = []\n",
    "        images = []\n",
    "        target_vs = []\n",
    "        target_ps = []\n",
    "        for g, i in game_pos:\n",
    "            image = g.make_image(i)\n",
    "            target_v, target_p = g.make_target(i)\n",
    "            target_v = np.array(target_v, dtype=np.float64)  # .reshape(-1, 1)\n",
    "            target_p = np.array(target_p)\n",
    "\n",
    "            # Augment Data\n",
    "            if np.random.random() < 0.5:\n",
    "                image = np.fliplr(image)\n",
    "                target_p = np.flip(target_p)\n",
    "\n",
    "            images.append(image)\n",
    "            target_vs.append(target_v)\n",
    "            target_ps.append(target_p)\n",
    "            # batch.append((image, target_v, target_p))\n",
    "        batch = [np.array(images), np.array(target_vs), np.array(target_ps)]\n",
    "        return batch\n",
    "\n",
    "\n",
    "class ResNet:\n",
    "    def __init__(self, weights=None):\n",
    "        self.rows = 6\n",
    "        self.columns = 7\n",
    "        self.model = self.create_model()\n",
    "        if weights:\n",
    "            self.model.load_weights(weights)\n",
    "\n",
    "    @staticmethod\n",
    "    def res_block(inputs, filters, reg=0.01, bn_eps=2e-5):\n",
    "        x = Conv2D(filters=int(filters), kernel_size=3, padding=\"same\", kernel_regularizer=l2(reg))(inputs)\n",
    "        x = BatchNormalization(epsilon=bn_eps)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2D(filters=int(filters), kernel_size=3, padding=\"same\", kernel_regularizer=l2(reg))(x)\n",
    "        x = BatchNormalization(epsilon=bn_eps)(x)\n",
    "        x = add([x, inputs])\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "    def policy_head(self, x, bn_eps=2e-5):\n",
    "        x = Conv2D(32, kernel_size=3, padding='same')(x)\n",
    "        x = BatchNormalization(epsilon=bn_eps)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(self.columns, activation='linear', name='policy_head')(x)\n",
    "        return x\n",
    "\n",
    "    def value_head(self, x, bn_eps=2e-5):\n",
    "        x = Conv2D(32, kernel_size=1, padding='same')(x)\n",
    "        x = BatchNormalization(epsilon=bn_eps)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(1, activation='tanh', name='value_head')(x)\n",
    "        return x\n",
    "\n",
    "    def create_model(self, num_residual_blocks=2, reg=0.01, bn_eps=2e-5):\n",
    "        inputs = Input(shape=(self.rows, self.columns, 4))\n",
    "        x = Conv2D(256, kernel_size=3, padding='same', kernel_regularizer=l2(reg))(inputs)\n",
    "        x = BatchNormalization(epsilon=bn_eps)(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        for _ in range(num_residual_blocks):\n",
    "            x = ResNet.res_block(x, 256)\n",
    "\n",
    "        p = self.policy_head(x)\n",
    "        v = self.value_head(x)\n",
    "\n",
    "        model = Model(inputs, [v, p])\n",
    "        return model\n",
    "\n",
    "    def inference(self, x):\n",
    "        if len(x.shape) != 4:\n",
    "            x = np.expand_dims(x, axis=0)\n",
    "        value, policy_logits = self.model.predict(x)\n",
    "        prob = tf.nn.softmax(policy_logits)\n",
    "        return value, prob\n",
    "\n",
    "class Game:\n",
    "    def __init__(self, history=None):\n",
    "        self.history = history or []\n",
    "        self.child_visits = []\n",
    "        self.rows = 6\n",
    "        self.columns = 7\n",
    "        self.num_actions = self.columns\n",
    "        self.initial_state = np.zeros((self.rows, self.columns))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.state())\n",
    "\n",
    "    def state(self, state_index=None):\n",
    "        board = self.initial_state.copy()\n",
    "        history = self.history\n",
    "        if state_index is not None:\n",
    "            history = history[:state_index]\n",
    "        for move in history:\n",
    "            token = -1 if len(np.argwhere(board != 0)) % 2 == 1 else 1\n",
    "            move_idx = self.lowest_position(board, move)\n",
    "            board[move_idx] = token\n",
    "        return board\n",
    "\n",
    "    @classmethod\n",
    "    def lowest_position(cls, board, col):\n",
    "        try:\n",
    "            row = np.argwhere(board[:, col] == 0)[-1]\n",
    "            return int(row), col\n",
    "        except IndexError:\n",
    "            return False\n",
    "\n",
    "    @property\n",
    "    def terminal(self):\n",
    "        to_play = self.to_play\n",
    "        if self.terminal_value(to_play) == 0:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def terminal_value(self, to_play, board_state=None):\n",
    "        reward = 1\n",
    "        state = self.state()\n",
    "\n",
    "        if board_state is not None:\n",
    "            state = board_state\n",
    "\n",
    "        if len(self.legal_actions) == 0:\n",
    "            return 1e-10\n",
    "\n",
    "        winners = [self.check_win_vert(state), self.check_win_horiz(state), self.check_win_diag(state)]\n",
    "\n",
    "        try:\n",
    "            consec = list(filter(None, winners))[0]  # 1 or -1\n",
    "            winner = 0 if consec == 1 else 1\n",
    "            return reward if winner != int(to_play) else -reward\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    @property\n",
    "    def legal_actions(self):\n",
    "        # returns np array of legal columns\n",
    "        try:\n",
    "            return np.array(list(filter(None, [self.lowest_position(self.state(), c) for c in range(7)])))[:, -1]\n",
    "        except IndexError:\n",
    "            return []\n",
    "\n",
    "    def clone(self):\n",
    "        return Game(list(self.history))\n",
    "\n",
    "    def apply(self, action):\n",
    "        if action not in self.legal_actions:\n",
    "            raise Exception('Illegal move')\n",
    "        self.history.append(action)\n",
    "\n",
    "    def store_search_statistics(self, root):\n",
    "        sum_visits = sum(child.visit_count for child in root.children.values())\n",
    "        self.child_visits.append([\n",
    "            root.children[a].visit_count / sum_visits if a in root.children else 0\n",
    "            for a in range(self.num_actions)\n",
    "        ])\n",
    "\n",
    "    def make_image(self, state_index=None):\n",
    "        state = self.state(state_index)\n",
    "        to_play = len(self.history[:state_index]) % 2\n",
    "        to_play_matrix = np.ones((self.rows, self.columns))\n",
    "        if to_play != 0:\n",
    "            to_play_matrix = -to_play_matrix\n",
    "            #state = state*-1\n",
    "            #state[state == -0] = 0\n",
    "        curr_player_binary = np.array(state == to_play_matrix, dtype='float64')\n",
    "        opp_player_binary = np.array(state == -to_play_matrix, dtype='float64')\n",
    "        input_image = np.dstack([state, curr_player_binary, opp_player_binary, to_play_matrix])\n",
    "        return input_image\n",
    "\n",
    "    def make_target(self, state_index: int):\n",
    "        #discount_rate = 0.95\n",
    "        # discount based on distance from terminal state\n",
    "        #move_dist = (len(self.history) - state_index)/2\n",
    "        value = self.terminal_value(state_index % 2)  # * discount_rate ** move_dist\n",
    "        return value, self.child_visits[state_index]\n",
    "\n",
    "    @property\n",
    "    def to_play(self):\n",
    "        # returns 0 if player 1 else returns 1\n",
    "        return len(self.history) % 2\n",
    "\n",
    "    @classmethod\n",
    "    def check_win_horiz(cls, board):\n",
    "        winner = None\n",
    "        win_con = 4\n",
    "        for row in board:\n",
    "            consecutive = []\n",
    "\n",
    "            for col in row:\n",
    "                if len(consecutive) == 0:\n",
    "                    consecutive = [col]\n",
    "                elif consecutive[-1] == col:\n",
    "                    consecutive.append(col)\n",
    "                else:\n",
    "                    consecutive = [col]\n",
    "                if len(consecutive) == win_con and consecutive[0] != 0:\n",
    "                    winner = consecutive[0]\n",
    "        return winner\n",
    "\n",
    "    @classmethod\n",
    "    def check_win_vert(cls, board):\n",
    "        winner = None\n",
    "        win_con = 4\n",
    "        for row in board.T:\n",
    "            consecutive = []\n",
    "            for col in row:\n",
    "                if len(consecutive) == 0:\n",
    "                    consecutive = [col]\n",
    "                elif consecutive[-1] == col:\n",
    "                    consecutive.append(col)\n",
    "                else:\n",
    "                    consecutive = [col]\n",
    "                if len(consecutive) == win_con and consecutive[0] != 0:\n",
    "                    winner = consecutive[0]\n",
    "        return winner\n",
    "\n",
    "    @classmethod\n",
    "    def check_win_diag(cls, board):\n",
    "        winner = None\n",
    "        win_con = 4\n",
    "        diags_lr = [board[::-1, :].diagonal(i) for i in range(-board.shape[0] + 1, board.shape[1])]\n",
    "        board_flip = np.fliplr(board)\n",
    "        diags_rl = [board_flip[::-1, :].diagonal(i) for i in range(-board.shape[0] + 1, board.shape[1])]\n",
    "        diags = diags_lr + diags_rl\n",
    "        for diag in diags:\n",
    "            if len(diag) >= 4:\n",
    "                consecutive = []\n",
    "                for d in diag:\n",
    "                    if len(consecutive) == 0:\n",
    "                        consecutive = [d]\n",
    "                    elif consecutive[-1] == d:\n",
    "                        consecutive.append(d)\n",
    "                    else:\n",
    "                        consecutive = [d]\n",
    "                    if len(consecutive) == win_con and consecutive[0] != 0:\n",
    "                        winner = consecutive[0]\n",
    "        return winner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, prior: float):\n",
    "        self.visit_count = 0\n",
    "        self.to_play = -1\n",
    "        self.prior = prior\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "\n",
    "    @property\n",
    "    def is_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visit_count\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Node Value: {self.value}\\n' \\\n",
    "               f'Node Prior: {self.prior}\\n' \\\n",
    "               f'Node Visits : {self.visit_count}'\n",
    "\n",
    "\n",
    "def run_mcts(config, game, network, add_noise=True):\n",
    "    root = Node(0)\n",
    "    evaluate(root, game, network)\n",
    "    if add_noise:\n",
    "        add_exploration_noise(config, root)\n",
    "\n",
    "    for _ in range(config.num_simulations):\n",
    "        node = root\n",
    "        scratch_game = game.clone()\n",
    "        search_path = [node]\n",
    "\n",
    "        while node.is_expanded:\n",
    "            action, node = select_child(config, node)\n",
    "            scratch_game.apply(action)\n",
    "            search_path.append(node)\n",
    "        value = evaluate(node, scratch_game, network)\n",
    "        backpropagate(search_path, value, scratch_game.to_play)\n",
    "    return select_action(config, game, root), root\n",
    "\n",
    "\n",
    "def select_action(config, game, root):\n",
    "    visit_counts = [(child.visit_count, action) for action, child in root.children.items()]\n",
    "    if len(game.history) < config.num_sampling_moves:\n",
    "        _, action = softmax_sample(visit_counts)\n",
    "    else:\n",
    "        _, action = max(visit_counts)\n",
    "    return action\n",
    "\n",
    "\n",
    "def select_child(config, node):\n",
    "    _, action, child = max((ucb_score(config, node, child), action, child)\n",
    "                           for action, child in node.children.items())\n",
    "    return action, child\n",
    "\n",
    "\n",
    "# The score for a node is based on its value, plus an exploration bonus based on\n",
    "# the prior.\n",
    "def ucb_score(config, parent, child):\n",
    "    pb_c = math.log((parent.visit_count + config.pb_c_base + 1) /\n",
    "                    config.pb_c_base) + config.pb_c_init\n",
    "    pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "\n",
    "    prior_score = pb_c * child.prior\n",
    "    value_score = child.value\n",
    "    return prior_score + value_score\n",
    "\n",
    "\n",
    "def evaluate(node, game, network):\n",
    "    value, policy_logits = network.inference(game.make_image())\n",
    "    value, policy_logits = np.squeeze(value), np.squeeze(policy_logits)\n",
    "    node.to_play = game.to_play\n",
    "    policy = {a: math.exp(policy_logits[a]) for a in game.legal_actions}\n",
    "    policy_sum = sum(policy.values())\n",
    "    for action, p in policy.items():\n",
    "        node.children[action] = Node(p / policy_sum)\n",
    "    return value\n",
    "\n",
    "\n",
    "def backpropagate(search_path: list, value: float, to_play):\n",
    "    for node in search_path:\n",
    "        node.value_sum += value if node.to_play == to_play else -value\n",
    "        node.visit_count += 1\n",
    "\n",
    "\n",
    "def add_exploration_noise(config, node: Node):\n",
    "    actions = node.children.keys()\n",
    "    noise = np.random.gamma(config.root_dirichlet_alpha, 1, len(actions))\n",
    "    frac = config.root_exploration_fraction\n",
    "    for a, n in zip(actions, noise):\n",
    "        node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
    "\n",
    "\n",
    "def softmax_sample(visit_counts):\n",
    "    visits, actions = zip(*visit_counts)\n",
    "    visits = np.array(visits).astype('float64')\n",
    "    prob = tf.nn.softmax(visits)\n",
    "    idx = np.random.choice(len(actions), p=prob)\n",
    "    return None, actions[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "class SmartRandomAgent:\n",
    "    ''' Makes random moves, unless it is one move away from victory, in which case, it will always choose that move\n",
    "           If opponent's next optimal move is a winning move, it will play the move that blocks the win'''\n",
    "\n",
    "    def __init__(self, dumb=False):\n",
    "        self.game = None\n",
    "        self.dumb = dumb\n",
    "\n",
    "    @property\n",
    "    def self_winning_move(self):\n",
    "        token = 1 if self.game.to_play == 0 else -1\n",
    "        move_col = None\n",
    "        for i in self.game.legal_actions:\n",
    "            copy_state = copy.deepcopy(self.game.state())\n",
    "            move = self.game.lowest_position(copy_state, i)\n",
    "            copy_state[move] = token\n",
    "            if self.game.terminal_value(self.game.to_play, copy_state) != 0:\n",
    "                move_col = i\n",
    "        return move_col\n",
    "\n",
    "    @property\n",
    "    def opp_winning_move(self):\n",
    "        token = 1 if self.game.to_play == 0 else -1\n",
    "        move_col = None\n",
    "        for i in self.game.legal_actions:\n",
    "            copy_state = copy.deepcopy(self.game.state())\n",
    "            move = self.game.lowest_position(copy_state, i)\n",
    "            copy_state[move] = -token\n",
    "            if self.game.terminal_value(-self.game.to_play, copy_state) != 0:\n",
    "                move_col = i\n",
    "        return move_col\n",
    "\n",
    "    def move(self, game):\n",
    "        if self.dumb:\n",
    "            game.apply(random.choice(game.legal_actions))\n",
    "            return game\n",
    "        self.game = game.clone()\n",
    "        opp_win_move = self.opp_winning_move\n",
    "        if opp_win_move is not None:\n",
    "            game.apply(opp_win_move)\n",
    "            return game\n",
    "        self_win_move = self.self_winning_move\n",
    "        if self_win_move is not None:\n",
    "            game.apply(self_win_move)\n",
    "            return game\n",
    "        game.apply(random.choice(self.game.legal_actions))\n",
    "        return game\n",
    "class Pit:\n",
    "    def __init__(self, agentZero, SmartRandomAgent, num_sims):\n",
    "        self.AI = agentZero\n",
    "        self.opp = SmartRandomAgent\n",
    "        self.num_sims = num_sims\n",
    "        self.AI_wins = 0\n",
    "        self.opp_wins = 0\n",
    "\n",
    "    def simulate(self):\n",
    "        # AI as player 1\n",
    "        for i in range(int(self.num_sims / 2)):\n",
    "            # print(i)\n",
    "            game = Game()\n",
    "            while not game.terminal:\n",
    "                game = self.AI.move(game)\n",
    "                if game.terminal:\n",
    "                    if len(game.legal_actions) != 0:\n",
    "                        self.AI_wins += 1\n",
    "                    break\n",
    "                game = self.opp.move(game)\n",
    "                if game.terminal:\n",
    "                    if len(game.legal_actions) != 0:\n",
    "                        self.opp_wins += 1\n",
    "                    break\n",
    "\n",
    "        # AI as player 2\n",
    "        for i in range(int(self.num_sims / 2)):\n",
    "            game = Game()\n",
    "            while not game.terminal:\n",
    "                game = self.opp.move(game)\n",
    "                if game.terminal:\n",
    "                    if len(game.legal_actions) != 0:\n",
    "                        self.opp_wins += 1\n",
    "                    break\n",
    "                game = self.AI.move(game)\n",
    "                if game.terminal:\n",
    "                    if len(game.legal_actions) != 0:\n",
    "                        self.AI_wins += 1\n",
    "                    break\n",
    "\n",
    "    @property\n",
    "    def winrate(self):\n",
    "        return self.AI_wins / self.num_sims * 100\n",
    "\n",
    "    @property\n",
    "    def winrate_opp(self):\n",
    "        return self.opp_wins / self.num_sims * 100\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'AI Wins : {self.AI_wins}'\n",
    "\n",
    "\n",
    "class AgentZeroCompetitive:\n",
    "    def __init__(self, config, net=None, mcts=False):\n",
    "        self.config = config\n",
    "        self.net = net\n",
    "        self.config.num_sampling_moves = 0\n",
    "        self.config.root_exploration_fraction = 0\n",
    "        self.mcts = mcts\n",
    "\n",
    "    def move(self, game, eval=False):\n",
    "        '''if eval == True:\n",
    "            action_M, root = run_mcts(self.config, game, self.net)\n",
    "            game.store_search_statistics(root)\n",
    "            print(f'MCTS Action: {action_M}')'''\n",
    "        if self.mcts:\n",
    "            action, root = run_mcts(self.config, game, self.net)\n",
    "            game.store_search_statistics(root)\n",
    "        else:\n",
    "            img = game.make_image()\n",
    "            val, prob = self.net.inference(np.expand_dims(img, axis=0))\n",
    "            prob = [p if idx in game.legal_actions else 0 for idx, p in enumerate(np.squeeze(prob))]\n",
    "            prob /= sum(prob)\n",
    "            print(f'{val=}')\n",
    "            print(prob)\n",
    "\n",
    "            action = np.argmax(prob)\n",
    "        game.apply(action)\n",
    "        return game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpath = r'/Users/timwu/models/AlphaZeroResNet/buffer.pkl'\n",
    "bpath2 = r'/Users/timwu/Desktop/buffer_before_epsilon.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(bpath, 'rb') as f:\n",
    "    b1 = pickle.load(f)\n",
    "with open(bpath2, 'rb') as f:\n",
    "    b2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3689"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b1.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b2.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = b1.buffer + b2.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  18, 5662, 3559, ..., 4568, 5203, 5554])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "idx = np.random.permutation(len(lst))\n",
    "lst = np.array(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, config, replay_buffer: ReplayBuffer, net):\n",
    "        self.config = config\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.net = net\n",
    "        self.learning_rate = self.config.learning_rate\n",
    "        self.total_epochs = 0\n",
    "\n",
    "    def run_selfplay(self, verbose=False):\n",
    "        game = Game()\n",
    "        while not game.terminal:\n",
    "            action, root = run_mcts(self.config, game, self.net)\n",
    "            game.apply(action)\n",
    "            game.store_search_statistics(root)\n",
    "        if verbose:\n",
    "            print(game)\n",
    "        self.replay_buffer.save_game(game)\n",
    "        return game\n",
    "\n",
    "\n",
    "    def train_network(self):\n",
    "\n",
    "        for i in range(self.config.batches_per_iter):\n",
    "            # get batch of data from replay_buffer\n",
    "            batch = self.replay_buffer.sample_batch()\n",
    "            self.update_weights(self.net.model, batch)\n",
    "\n",
    "    def update_weights(self, network, batch):\n",
    "        # compile network with most recent learning rate\n",
    "        network.compile(loss=[tf.nn.softmax_cross_entropy_with_logits, 'mean_squared_error'],\n",
    "                        optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate))\n",
    "\n",
    "        images, target_v, target_p = batch\n",
    "\n",
    "        network.fit(x=images, y=[target_v, target_p], epochs=self.config.epochs_per_batch)\n",
    "\n",
    "        return\n",
    "\n",
    "    def train_ep(self, num_self_play_games=None):\n",
    "        if num_self_play_games is None:\n",
    "            num_self_play_games = self.config.self_play_games\n",
    "        for i in range(num_self_play_games):\n",
    "            print(f'SelfPlay Game: {i + 1}')\n",
    "            if i % 10 == 0:\n",
    "                self.run_selfplay(verbose=True)\n",
    "            else:\n",
    "                self.run_selfplay()  # self play games added to replay buffer\n",
    "\n",
    "        self.train_network()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentZeroCompetitive(AlphaZeroConfig(),\n",
    "                             net,\n",
    "                             mcts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "opp = SmartRandomAgent(dumb=True)\n",
    "pit = Pit(agent, opp, num_sims=20)\n",
    "pit.simulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AI Wins : 12"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AI Wins : 11"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.history[:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  1.,  1., -1.,  1.],\n",
       "       [ 0.,  0.,  0., -1., -1.,  1., -1.],\n",
       "       [ 0.,  0., -1.,  1.,  1., -1.,  1.],\n",
       "       [ 0.,  0.,  1., -1.,  1.,  1., -1.],\n",
       "       [ 0.,  0., -1.,  1., -1., -1.,  1.],\n",
       "       [ 0.,  0.,  1., -1.,  1., -1., -1.]])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.make_image(28)[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow_addons.optimizers.weight_decay_optimizers import DecoupledWeightDecayExtension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NadamW(DecoupledWeightDecayExtension, tf.keras.optimizers.Nadam):\n",
    "    def __init__(self, lr, weight_decay, *args, **kwargs):\n",
    "        super(NadamW, self).__init__(lr, weight_decay, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = tf.Variable(0, trainable=False)\n",
    "schedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    [10000, 15000], [1e-0, 1e-1, 1e-2])\n",
    "# lr and wd can be a function or a tensor\n",
    "lr = 1e-1 * schedule(step)\n",
    "wd = lambda: 1e-4 * schedule(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'/Users/timwu/Desktop/traininglog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path,'r') as f:\n",
    "    x = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i.split('\\nloss=') for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-a4a7134d7f8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1234567\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in x:\n",
    "    if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2278 - value_head_'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [i.split(' - ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4797\\nloss=4.489809989929199\\n  0%|                                     | 3/3000 [09:15<146:19:54, 175.77s/it]New Games: 10\\nEpoch : 2, generating batch on 5000 games\\nSun Nov 29 19:17:47 2020\\nlearning_rate=0.0009801\\nMean Game Len: 28.5896\\n128/128 [==============================] - 38s 284ms/step - '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
